% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[final]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Real Time Domain Adaptation in Semantic Segmentation}

\author{
Elena Buccoliero\\ 
Politecnico di Torino\\ 
s317320\\  
{\tt\small s317320@studenti.polito.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Alessandro Chiabodo\\
Politecnico di Torino\\ 
s309234\\
{\tt\small s309234@studenti.polito.it}
\and
Beatrice Occhiena\\ 
Politecnico di Torino\\ 
s314971\\  
{\tt\small s314971@studenti.polito.it}
}
\maketitle

%%%%%%%%% ABSTRACT %%%%%%%%%
\begin{abstract}
    This paper explores the realm of real-time domain adaptation in semantic segmentation, focusing on the implementation and comparison of two domain adaptation algorithms. The main objective is to understand the challenges of adapting semantic segmentation networks to various domains and to evaluate different approaches. The project involves replicating experiments, implementing data augmentation and domain adaptation techniques, and testing variations to improve overall performance. The code implementation of the project is available at: \url{https://github.com/AChiabodo/AML_Semantic_DA}.
\end{abstract}

%%%%%%%%% BODY TEXT %%%%%%%%%
\section{Introduction}
\label{sec:intro}

Semantic segmentation is a key task in computer vision that enables machines to understand visual scenes at a pixel level by assigning semantic labels to each pixel. 
In dynamic environments, where scenes evolve rapidly, achieving real-time semantic segmentation necessitates robust adaptation to different domains. This adaptation is crucial because the characteristics of the visual data may vary significantly between different environments due to factors such as lighting conditions, weather, and camera perspectives.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{fine_semantic_label.png}
    \caption{Example of semantic labels for a real-world street image.}
    \label{fig:fine_semantic_label}
\end{figure}

The primary goal of this project is to is to explore the problem of real-time semantic segmentation and propose solutions to cope with the domain shifts that occur when the data distribution in the training domain (where the model is trained) differs from the data distribution in the target domain (where the model is applied). These domain shifts can negatively affect the performance of semantic segmentation models, resulting in errors and inaccuracies in the segmentation outputs.

To tackle these challenges, in our project we implement and compare two different domain adaptation techniques that aim to reduce this gap and enhance the model’s ability to generalize to unseen target domain data. The first technique is Unsupervised Adversarial Domain Adaptation, which trains a generator and a discriminator to learn domain-invariant features. The second one is Fourier Domain Adaptation, which achieves a simpler image-to-image style translation.

Besides the theoretical aspects, the project also considers the practical challenges of real-time semantic segmentation, such as computational efficiency and training complexity. We aim to develop lightweight segmentation models that can achieve real-time performance while maintaining high accuracy, which is crucial for practical applications in domains such as autonomous driving, robotics, and augmented reality.


%------------------------------------------------------------------------
\section{Related Works }

Before delving into the project details, it is essential to review related works in the field of semantic segmentation, real-time networks, domain adaptation and implementation of various approaches.

%-------------------------------------------------------------------------
\subsection{Semantic Segmentation}

As cited in \cite{SemanticSeg}, the field of 2D semantic segmentation has seen substantial improvements due to the advent and application of deep learning methodologies. These advancements have revolutionized the way we label each pixel in an image to accurately identify objects and their boundaries, thereby enhancing our understanding and interpretation of visual data.

State-of-the-art methodologies heavily depend on deep neural networks, with innovations such as the transformation of classification CNNs into FCNs. The training of these advanced models typically necessitates dense pixel annotations, leading to the exploration of weakly and semi-supervised approaches to mitigate labeling costs. Synthetic datasets like GTA5 and SYNTHIA have been developed to alleviate annotation burdens, but models trained exclusively on synthetic data often face difficulties in generalizing to real-world scenarios due to domain shifts. To counteract this, domain adaptation techniques are utilized to effectively align synthetic and real-world image domains. Despite the challenges, ongoing efforts are aimed at synthesizing more realistic images and narrowing the domain gap, ultimately enhancing the performance of semantic segmentation models across diverse environments.

%-------------------------------------------------------------------------
\subsection{BiSeNet}

BiSeNet \cite{BiS}, renowned for its two-stream architecture balancing spatial detail and contextual insight, has recently been critiqued for its computational requirements and dependency on pretrained backbones. In response, the STDC-Seg network \cite{RealTimeBiSeNet} emerges as a refined alternative, replacing BiSeNet's backbone and Spatial Path with the innovative Short-Term Dense Concatenate Module. This evolution streamlines real-time semantic segmentation, enhancing efficiency while maintaining performance. Experimental results, particularly on datasets like Cityscapes, affirm STDC-Seg's superiority over BiSeNet in speed and accuracy. While BiSeNet paved the way, STDC-Seg's advancements signal progress, pointing toward more efficient and effective real-time semantic understanding.

%-------------------------------------------------------------------------
\subsection{Unsupervised Domain Adaptation}

Domain adaptation addresses the issue of disparate data distributions between the training and testing domains in machine learning \cite{CD}. Traditional methods typically assume identical distributions, but real-world scenarios often exhibit variations due to a variety of data sources or outdated training data. Unsupervised domain adaptation seeks to resolve this by aligning feature distributions in the absence of labels in the target domain.\\
In the realm of image classification, strategies are employed to alleviate domain-shift problems by aligning features. Techniques such as PixelDA augment target datasets using source images. However, domain adaptation for pixel-level tasks like semantic segmentation is still under development.  Recent endeavors have introduced adversarial learning techniques, such as DANN, for fully-convolutional adaptation, or have concentrated on synthetic-to-real or cross-city adaptation with class-wise adversarial learning. This study \cite{DomAd} puts forth an efficient domain adaptation algorithm that utilizes adversarial learning in the output space, specifically designed for pixel-level tasks like semantic segmentation, which necessitate the preservation of spatial and local information.

%-------------------------------------------------------------------------
\subsection{Fourier Domain Adaptation}

\begin{figure}[h]
  \centering
   \includegraphics[width=0.9\linewidth]{FDA.png}
   \caption{Spectral Transfer: Mapping a source image to a target “style” without altering semantic content. A randomly sampled target
image provides the style by swapping the low-frequency component of the spectrum of the source image with its own. The outcome, a source
image in target style, shows a smaller domain gap perceptually.
   }
   \label{fig:fda}
\end{figure}

Fourier Domain Adaptation (FDA) \cite{fda} is an additional unsupervised domain adaptation technique in which the discrepancy between the source and target distributions is reduced by swapping the low frequency spectrum  of the source images with the corresponding spectrum of the target images. 
This process is accomplished through the application of the Fourier Transform and its corresponding inverse.
Contrary to numerous state-of-the-art methods that require complex adversarial training, FDA does not require any training to perform domain alignment. Despite its simplicity, it achieves state-of-the-art performance on current benchmarks when integrated with a standard semantic segmentation model. 
In addition, FDA introduces a robust entropy minimization regularization and a multi-band transfer scheme to further enhance its performance. This multi-scale approach, which averages the predictions from models trained with different spectral neighborhood sizes, has proven to be particularly effective.\\



%------------------------------------------------------------------------
\section{Method}
\label{sec:formatting}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{Domain Adaptation.png}
   \caption{
   Algorithmic overview. Given images of size HxW in the source and target domains, we pass them through the segmentation network in order to obtain predictions of the output. For source predictions with C categories, a segmentation loss is calculated based on the source ground truth. To make the target predictions closer to the source predictions, we use a discriminator to distinguish whether the input is from the source or target domain. Then, an adversarial loss is computed on the target prediction and backpropagated to the segmentation network. We refer to
this process as an adaptation module, and we illustrate our proposed multi-level adversarial learning with two adaptation modules at two different levels}
   \label{}
\end{figure*}

TODO !!!!

In this project, to address the challenge of semantic segmentation, we utilize BiSeNet, a cutting-edge architecture renowned for its effectiveness in pixel-level tasks.
BiSeNet \cite{BiS} stands out due to its innovative two-stream architecture, strategically designed to seamlessly integrate spatial details and contextual information. This dual-path structure comprises a Spatial Path, responsible for capturing fine-grained spatial features, and a Context Path, which aggregates contextual information from a lightweight classification backbone. By leveraging these complementary streams, BiSeNet can achieve accurate and detailed semantic segmentation results.

One of the notable aspects of BiSeNet is its ability to balance performance and computational efficiency. While previous approaches may have struggled with computational intensity or reliance on pretrained classification backbones, BiSeNet addresses these concerns by optimizing its architecture for enhanced efficiency. However, to further refine BiSeNet and overcome its limitations, our project introduces a strategic refinement known as the Short-Term Dense Concatenate (STDC) network \cite{RealTimeBiSeNet}.

STDC-Seg, the refined version of BiSeNet, replaces the classification backbone and Spatial Path with a novel Short-Term Dense Concatenate Module. This module not only extracts deep features with a scalable receptive field but also seamlessly incorporates multi-scale information. By doing so, STDC-Seg streamlines the segmentation process, eliminating redundancy, and improving computational efficiency without compromising performance.

In our project, the utilization of BiSeNet and its refinement through STDC allows us to achieve good results in real-time applications. By leveraging the strengths of BiSeNet's architecture and enhancing its efficiency through strategic refinements, we can effectively tackle the challenges posed by semantic segmentation tasks.



%---------------------------------------------------------------------------

\subsection{Unsupervised Adversarial Domain Adaptation}

TODO !!!! Check if correct LOSS functions. CROSS ENTROPY LOSS missing??? **************************

Following \cite{DomAd}, we implement a domain adaptation method based on single-level generative adversarial training, using two modules: a generator $G$ that is our segmentation model, and a discriminator $D$ that is a fully convolutional classifier.

We start by train our model on the source images with labels, as usual. Then we also feed the model the target images without annotations, which can not help the model improve without the adversarial training. At this point, we use the predictions from both domains as inputs for the discriminator, which tries to tell them apart. The discriminator’s feedback helps the model learn to generate similar predictions for both domains, according to the adversarial loss Eq.(\ref{eq:domAd_adversarial_loss}).

For the segmentation network training, the total loss is given by Eq.(\ref{eq:domAd_total_loss}), which combines the cross-entropy loss Eq.(\ref{eq:domAd_cross_entropy}), used during the training of the model on the source images, and the adversarial loss Eq.(\ref{eq:domAd_adversarial_loss}), used to fool the discriminator and train the model to bring the two domains closer together.
Note that in Eq.(\ref{eq:domAd_total_loss}) we use $\lambda_{\text{adv}}$ to balance the two losses during training.\\

\begin{equation}
L_(I_s,I_t) =L_{\text{seg}}(I_s) + \lambda_{\text{adv}} L_{\text{adv}}(I_t)
\label{eq:domAd_total_loss}
\end{equation}
\begin{equation}
L_{\text{seg}}(I_t) = -\sum_{h,w} \sum_{c \in C} Y_s^{(h,w,c)} \log(P_s^{(h,w,c)})
\label{eq:domAd_cross_entropy}
\end{equation}
\begin{equation}
L_{\text{adv}}(I_t) = -\sum_{h,w} \log(D(P_t)^{(h,w,1)})
\label{eq:domAd_adversarial_loss}
\end{equation}

Here are the descriptions of the two components in the adversarial training:

\textbf{Generator}: A segmentation model that performs well on the source domain and tries to fool the discriminator by making its predictions on the target domain look like they are from the source domain, thus reducing the domain gap. Unlike \cite{DomAd}, we use BiSeNet as segmentation network, which provides a fast speed and a good accuracy.

\textbf{Discriminator:} a binary classifier that tries to distinguish between the predictions of the generator on the source and target domains. We follow \cite{DomAd} and used a fully convolutional network with 5 convolutional layers with a $4x4$ kernel and stride 2, each followed by a leaky ReLU (except for the last layer). Then we add an upsampling layer to match the output size with the input size.

%-----------------------------------------------------------------------

\subsection{Fourier Domain Adaptation}

In the proposed Fourier Domain Adaptation (FDA) method\cite{fda}, the first stage involves the application of the Fourier Transform to enhance the spectral properties of the source images from GTA5.

The parameter $\beta$ is introduced to control the size of the spectral neighborhood used in the FDA process. This parameter plays a key role in determining the extent of the low-frequency amplitude components of the source images that are swapped with those of the target images. Importantly, while this swapping process occurs, the phase component, which contains the majority of the semantic content, is preserved. We can also note that $\beta$ is not determined by pixel measurements, so its selection is independent of the image’s size or resolution

Once the source images are adapted to the target domain using FDA, we train a segmentation network $\phi_w$ on the adapted source dataset $D_{s \rightarrow t}$. 

Since FDA aligns the source and target domains in the spectral space, the unsupervised domain adaptation (UDA) problem is effectively reduced to a semi-supervised learning problem. Therefore, the training process uses a standard cross-entropy loss $L_{ce}$ in conjunction with an entropy regularization term $L_{ent}$ that penalizes the decision boundary crossing regions densely populated by target data points. This term is weighted by a robust Charbonnier penalty function $\rho(x) = (x^2 + 0.001^2)^{\eta}$, that favors high entropy predictions.

The overall loss function for training the network is then given by:
\begin{equation}
  L(\phi^w; D^{s \rightarrow t}, D^t) = L_{ce}(\phi^w; D^{s \rightarrow t}) + \lambda_{ent} L_{ent}(\phi^w; D_t)
\label{eq:overall_loss}
\end{equation}
where $\lambda_{ent}$ is a hyperparameter that controls the weight of the entropy regularization term.\\

In the second stage, we apply a self-supervised training technique based on Multi-band Transfer (MBT) to further improve the performance of the segmentation network. MBT leverages the predictions of multiple networks $\phi_w^{\beta}$ that are trained with different values of $\beta$ in the FDA process. The intuition behind MBT is that different values of $\beta$ will result in different levels of domain alignment and semantic preservation, and thus different segmentation outputs.

\begin{equation}
    \hat{y}^t_i = arg max \frac{1}{M} \sum
\label{eq:mbt_eq}
\end{equation}

By averaging the predictions of these models using Eq.(\ref{eq:mbt_eq}), where M is the number of instantiated networks, MBT can generate reliable pseudo-labels for the training split of the target dataset. These pseudo-labels are then used to further train the networks in a self-supervised manner, leveraging the following loss:

\begin{equation}
\begin{split}
    L_{sst}(\phi^w; D^{s \rightarrow t}, \hat{D}_t) = L_{ce}(\phi^w; D^{s \rightarrow t})\\
    + \lambda_{ent} L_{ent}(\phi^w; D_t)+ L_{ce}(\phi^w; \hat{D}_t)
\end{split}
\label{eq:SS_training_loss}
\end{equation}
where $\hat{D}_t$ is ${D}_t$ augmented with the pseudo-labels.


%------------------------------------------------------------------------
\section{Experiments}
In this section we present a comprehensive evaluation of the segmentation network’s performance on the target dataset at each stage. We apply multiple data augmentation techniques to the synthetic dataset to enhance cross-domain performance, setting the stage for the primary task of domain adaptation.

\subsection{Datasets}

\textbf{GTA5}: This dataset \cite{GroundTruth} consists of 24,966 computer-generated images extracted from a video game. For training our model, we use 2,000 images and reserve 500 for testing. The original image size is 1914x1052.

\textbf{Cityscapes}: This dataset \cite{City} comprises approximately 5,000 finely annotated images of German cities, with a focus on semantic understanding of urban street scenes. For performance reasons, we utilize around 1,572 for training and 500 for testing our model. The original size of the images is 2048x10243.

In both cases, images are resized to half of their original size in order to reduce computational cost and to ensure that the input data to the network is within a similar scale.

\subsection{Training Details}
All training is conducted for 50 epochs on both CoLab and personal devices. The results presented in the paper are derived from the same source to avoid any device-related discrepancies.\\

TODO !!!! CHECK BELOW

To train BiSeNet, we used SGD with a momentum of 0.9, a weight decay of $1e^{-4}$, and an initial learning rate of 0.01, which was adjusted following the 'poly' learning rate scheduler. For the discriminator used in Domain Adaptation, we used an Adam optimizer with a learning rate of 0.001 and a 'poly' learning rate scheduler.

For the backbone of our BiSeNet model we use STDCNet813 initialized with the pretrained weights on ImageNet taken from the authors of \cite{RealTimeBiSeNet}.\\

\textbf{Preprocessing}: During the preprocessing stage of all our experiments, we employ data normalization and rescale the tensor values from the range [0, 255] to [0.0, 0.1]. Apart from FDA, which utilizes the mean of the Cityscapes dataset for implementation purposes, we use the mean and variance from the ImageNet dataset. This approach is adopted to ensure that the input data to the network falls within a similar scale, thereby enhancing the stability and speed of the network training process. It also improves learning dynamics by ensuring consistent gradient scales across all layers, adds a regularization effect reducing overfitting and prevents saturation of activation functions.

\subsection{Initial Training}

Following the project requirements, we first evaluate the capabilities of BiSeNet by training our model on the training split of one dataset and testing it on the validation split of the same. The results of this first round of training are listed in Tab. \ref{tab:result}.

Specifically, to train on Cityscapes, we use the fine-grained labels provided by \cite{City}, which have 19 distinct semantic classes. As expected, the model performs quite well when we test it on data from the same domain it was trained on, since there is no domain shift. Therefore, we consider this result our upper bound for the following phases of the project.\\

After training on GTA5 and assessing the performace of the model, we start a cross-domain evaluation of the model trained on this synthetic dataset on Cityscapes. At this point the results change drastically: we observe a significant drop in performance, which highlights the presence of a large domain shift between the two datasets. The model trained on GTA5 cannot generalize well to the real-world domain, because of the differences in visual appearance, scene layouts, and other domain-specific factors.

The accuracy and mIoU both plummet (see Tab. \ref{tab:result}), and the model often fails to recognize basic structures in the test images, especially the fine details involving people, traffic lights, and other small objects.

\subsection{Data Augmentation}

We first use different data enhancement techniques to reduce the domain discrepancy as much as possible, which improves the initial performance. We try two different types of data augmentation, each with a 50\% chance of being applied to an image from the GTA5 dataset.

The first type, referred to as weak data augmentation, involves enlarging and cropping the images, as well as flipping them horizontally. This is done to concentrate on the finer details within the synthetic images.

The second type, strong data augmentation, includes all the operations listed above, with the addition of blurring and color jittering. The purpose of this is to enhance the diversity of the images and to minimize the domain gap between the GTA5 and CityScapes datasets.\\

As underlined by the results listed in Tab. \ref{tab:result}, we observe that the alteration of color and saturation transformations notably enhances the overall performance. This underscores the substantial disparities, particularly in the color domain, between the two datasets utilized. Consequently, we anticipate that a style-transfer method such as FDA will be an optimal solution for our specific scenario. 

\subsection{Unsupervised Adversarial Domain Adaptation}

TODO !!!!

We start to perform adversarial training with labelled synthetic data (source domain) GTA5 and unlabelled real-word data (target domain) Cityscapes.

Slightly better results were obtained with Adversarial Domain Adaptation, but the instability of the training and the general complexity, combined with the not very good results in our case, led us to look for valid alternatives.

\subsection{FDA with Single Scale}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{fda05.png}
    \caption{Example of FDA style-transfer for $\beta$ = 0.05.}
    \label{fig:fdastylef}
\end{figure}

Following the example of \cite{fda}, we instantiate three BiSeNet segmentation networks with $\beta = 0.01, 0.05, 0.09$, and train them separately using Eq. \eqref{eq:overall_loss}. We set $\lambda_{ent} = 0.005$ and $\eta = 2.0$ for all experiments.
Choosing an appropriate value for $\beta$ is crucial as it can affect the quality of the domain adaptation. A small $\beta$ means only a small portion of the low-frequency components are swapped, which may result in insufficient adaptation. Conversely, a large $\beta$ may cause too much distortion in the source images and loss of semantic information, with visible artifacts.

It’s important to highlight that, unlike previous stages, in Fourier Domain Adaptation (FDA), normalization is applied only after the domain shift has been performed. This is due to the fact that the Fast Fourier Transform (FFT) algorithm, which is used in FDA, is numerically stable for non-negative values. Applying normalization prior to the transformation could result in large areas of artifacts. As such, images are maintained within the range [0, 255] before the transformation.

After the FDA process, the images are normalized and rescaled to the range [0, 1] for the purpose of training the segmentation network. In this context, the normalization is carried out based on the mean value of the Cityscapes dataset. The reason for this is that FDA specifically aims to align the source (GTA5) and target (Cityscapes) domains in the feature space.

\begin{table}[b]
    \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
    $\beta$ & Accuracy(\%) & mIoU(\%) \\
    \midrule
        0.01 & 69.8 & 30.4 \\
        0.05 & 70.8 & 29.4 \\
        0.09 & 72.9 & 32.6 \\
        \bottomrule
    \end{tabular}
    \caption{Training results of Single Scale FDA with various $\beta$ and weak data augmentation.}
    \label{tab:fda_single_scale}
\end{table}

We experiment using both non-augmented images and weakly augmented ones, which are enlarged and then cropped to match the size of the Cityscapes images. During this phase, we deliberately avoid strong data augmentation techniques that modify style attributes such as color and contrast, to prevent any interference with the effective style transfer of FDA.
Our observations indicate that the weakly augmented runs yield better performance. This is likely because the model can concentrate more on finer details, such as humans, poles, traffic lights, and so forth. Consequently, we decide to maintain this setting for subsequent steps. Our best results are collected in Tab. \ref{tab:fda_single_scale}.

\subsection{Multi-band Transfer (MBT)}

We conduct an evaluation of the averaged predictions from our three trained models on the Cityscapes Validation Set. This results in an additional improvement, as reported in Tab. \ref{tab:result}. The expected enhancement in performance is due to the observation that no single network consistently surpasses others across all semantic classes, regardless of the variations in the value of $\beta$.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{FDASSL.png}
    \caption{Example of a RGB generated pseudo-label.}
    \label{fig:pseudolabel}
\end{figure}

In the second phase, our focus shifts to the generation of pseudo-labels for the target Training Set, utilizing the predictions from the model adapted with Multi-band Transfer (MBT). We address the self-referential problem, which arises when pseudo-labels are used as ground truth, leading to potential overfitting and confirmation bias as the networks learn from their own predictions.

While MBT already offers a form of regularization by leveraging spectral diversity, we further enhance this by implementing a thresholding method. This method filters out low-confidence predictions from the pseudo-labels, accepting only those predictions that are within the top 66\% or exceed a confidence level of 0.9 for each semantic class.

As depicted in Fig. \ref{fig:pseudolabel}, the black areas located along the borders of each object represent regions where the model’s predictions are less confident. To prevent these uncertain areas from influencing the learning process, we assign them a label of 255, which corresponds to the “ignored class”. This ensures that the model’s training is not adversely affected by these low-confidence predictions.

\begin{table*}[t]
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method & Accuracy & mIoU & Epoch Time & Batch Size & Dataset\\
    \midrule
    Cityscapes & 80 & 53.4 & 1:15 & 20 & CS$\rightarrow$CS \\
    GTA5 & 75.8 & 47.8 & 1:35 & 20 & GTA$\rightarrow$GTA\\
    Un-Adapted Cross Domain & 44.8 & 17.9 & --- & - & GTA$\rightarrow$CS\\
    CD Data Augmentation 1 & 43.3 & 17.2 & 1:21 & 10 & GTA$\rightarrow$CS\\
    CD Data Augmentation 2 & 39.6 & 26.4 & 1:40 & 10 & GTA$\rightarrow$CS\\
    Adversarial DA (augm2) & 72.1 & 30.9 & 2:40 & 10 & GTA,CS$\rightarrow$CS\\
    Fourier DA ($\beta=0.09$) & 72.9 & 32.6 & 1:34 & 10 & GTA,CS$\rightarrow$CS\\
    Fourier DA (MBT) & 73.1 & 33.0 & --- & - & GTA,CS$\rightarrow$CS\\
    Fourier DA (SSL $\beta=0.01$) & 75.7 & 36.7 & 1:36 & 10 & GTA,CS$\rightarrow$CS\\
    Fourier DA (SSL + MBT) & 75.9 & 37.5 & --- & - & GTA,CS$\rightarrow$CS\\
    \bottomrule
  \end{tabular}
  \caption{Training results of all our project}
  \label{tab:result}
\end{table*}

\subsection{Self-supervised Learning FDA}

We instantiate other three new segmentation networks with $\beta = 0.01, 0.05, 0.09$, and train them separately using Eq. \eqref{eq:SS_training_loss}.

\begin{table}[h]
  \centering
  \begin{tabular}{@{}lccc@{}}
  \toprule
  $\beta$ & Accuracy(\%) & mIoU(\%) \\
  \midrule
    0.01 & 75.7 & 36.7 \\
    0.05 & 75.2 & 36.4 \\
    0.09 & 73.8 & 31.5 \\
    \bottomrule
  \end{tabular}
  \caption{Training results of SSL FDA with various $\beta$ and weak data augmentation.}
  \label{tab:ss_fda}
\end{table}

Upon examining the relative improvement of each $\phi^w_{\beta}$, we observe a trend similar to that reported by the authors of \cite{fda}. Specifically, as reported in Tab. \ref{tab:fda_single_scale} and Tab. \ref{tab:ss_fda}, the best performer in the initial training round is $\phi^w_{{\beta} = 0.09}$. However, this model becomes the least effective during the first Self-supervised Learning round.\\
We hypothesize that using a smaller value for $\beta$ will lead to fewer changes in the adapted source dataset, making it less likely to match the target dataset compared to when a larger $\beta$ is used. However, when we use pseudo-labels to bring the two datasets closer together, the adapted source dataset will be less biased because it’s closer to the target dataset and has less variation.\\
Lastly, we aim to further boost performance by conducting an extra evaluation using Multi-band Transfer (MBT) and the outcomes of Self-Supervised Learning (SLL).










%------------------------------------------------------------------------
\section{Conclusion}

TODO !!!! CHECK BELOW

In this study, we investigated the efficacy of various deep learning techniques for semantic segmentation across datasets and domains. Our comprehensive analysis, as summarized in the table of results Table \ref{tab:result}, sheds light on the performance and efficiency of different models under diverse conditions.\\
First, we evaluated our models on the Cityscapes dataset, a widely used benchmark in the field of semantic segmentation. Our results demonstrate good accuracy and average Intersection over Union (mIoU) scores of 80.0\% and 53.4\%, respectively.
A second benchmark was performed on the synthetic dataset GTA5, which contains urban scenes extracted from the game of the same name, obtaining in this case an accuracy of 75.8\% and an mIoU of 47.8\%
\\%Come ti sembra? no aspetta
To evaluate the model's performance in cross-domain scenarios, we first tested the effect of domain shifting without any type of optimization, and found that the model's performance was severely degraded by domain shifting.\\
We also investigated the impact of data augmentation techniques and domain adaptation on model performance. Augmenting the Cross Domain dataset with various techniques yielded mixed results, with Augmented CD 2 showing improved mIoU compared to its predecessor. \\
In addition, the use of domain adaptation strategies such as Generative Adversarial Training and Fourier Domain Adaptation significantly improved the performance of the model, in particular the use of FDA combined with Self Supervised Learning with the creation of pseudo-labels of the target images allowed us to achieve remarkable results, obtaining a mIoU of 36.7\% and an accuracy of 75.7\%.\%. \\
In conclusion, by leveraging state-of-the-art techniques and conducting rigorous experiments, we demonstrate the potential of deep learning in addressing complex vision tasks and advancing real-world applications in urban scene understanding and beyond.



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}

}

\end{document}
